---
title: 'Random Forests'
output: github_document
---

Ensure that you have completed `1-Introduction.Rmd` before running this notebook.

```{r setup, include = F}
### Knit Options ###
knitr::opts_knit[['set']](
  base.dir = '~/Git/Wearables_Activity_Recognition/',
  root.dir = '~/Git/Wearables_Activity_Recognition/'
)

### Chunk Options ###
knitr::opts_chunk[['set']](
  echo = T,
  comment = '',
  dpi = 600,
  warning = F,
  message = F,
  fig.path = 'figures/'
)

### General Options ###
options(
  digits = 7, # Default = 7
  scipen = 0, # Default = 0
  width = 120 # Default = 80
)
```


## Setup

Install required packages:
```{r install-packages, results = 'hide'}
### Required Packages ###
pkgs <- c(
  'data.table', # Data import/management.
  'randomForest' # Random forest.
)

### Packages to Install ###
pkgs <- pkgs[!{pkgs %in% rownames(installed.packages())}]

### Install (if Required) ###
install.packages(pkgs)

### Tidy Up ###
rm(pkgs)
```

Load required package namespaces:
```{r packages, results = 'hide'}
### Packages ###
library(data.table)
library(randomForest)
```

Define some useful path variables:
```{r paths}
### Paths ###
## Capture-24 Directory ##
dir_cap24 <- paste(here(), 'data/capture24sample', sep = '/')
zip_cap24 <- paste(here(), 'data/capture24sample.zip', sep = '/')
```

### Data

In the previous notebook, we extracted features from 30 second windows of the first participants raw data.
The extracted features for 30s windows of accelerometry taken from all participants can be read from the `feats_all.csv.gz` file.
```{r}
### Data for all participants ###
dt_feat <- fread(
  file = file.path(dir_cap24, 'feats_all.csv.gz'), # File path.
  sep = ',', # Delimiter.
  na.strings = '' # Missing data encoding.
)
```

Before continuing, ensure you understand the meaning of each column in this data table, and their contents match your expectations.
```{r features}
### First 2 Rows Per Label ###
dt_feat[, head(.SD, 2), by = label]
```

### Training on first participant

Let's first train a random forest model only on the first participants data.
Random forests are an ensemble learning method in which the results of multiple decision trees are aggregated to identify the most popular result. We can fit a random forest to our extracted feature set using the following code.
```{r first-participant-training}
dt_feat_p001 <- dt_feat[dt_feat$participant==1, ]

### Predictors ###
predvars <- c("x_mean", "x_sd", "y_mean", "y_sd", "z_mean", "z_sd", "v_mean", "v_sd")

### Set Seed ###
set.seed(42)

### Random Forest Fit ###
fit_rf <- randomForest(
  # Training Data #
  x = dt_feat_p001[, ..predvars],
  y = as.factor(dt_feat_p001[, label]),
  # Model Params #
  strata = label, # Factor variable used for stratified sampling.
  importance = T, # Should variable importance be assessed?
  keep.forest = T # Keep final forest for predictions.
)

### Confusion Matrix ###
fit_rf
```
Note that OOB estimate of error rate gives us a estimate cross validation accuracy = 100 - 7.51% = 94.49%.
This is very high, and it appears we have created a good model identifying activity.
However, can you identify some faults with this model so far?

Now let's try training the model once again on the first participant, but this time testing on a second participant.
How well do we expect our model to perform?
```{r first-participant-training}
dt_feat_p002 <- dt_feat[dt_feat$participant==2, ]

set.seed(42)

### Random Forest Fit ###
fit_rf <- randomForest(
  # Training Data #
  x = dt_feat_p001[, ..predvars],
  y = as.factor(dt_feat_p001[, label]),
  # Test Data #
  xtest = dt_feat_p002[, ..predvars],
  ytest = as.factor(dt_feat_p002[, label]),
  # Model Params #
  strata = label, # Factor variable used for stratified sampling.
  importance = T, # Should variable importance be assessed?
  keep.forest = T # Keep final forest for predictions.
)

### Confusion Matrix ###
fit_rf
```
While the OOB estimate of error rate remains quite low, the test error rate is very high.
Our model trained only on one participant failed to generalise on our second participant.
What else do you notice when you compare the confusion matrices between the training set and test set?

### Train/test split

Instead of training and testing on only one participant, let's split the learning data into 2/3 training and 1/3 test sets, containing multiple participants.

```{r}
### Split Data into Training & Test Sets ###
## Data Split ##
dt_train <- dt_feat[dt_feat$participant <= 100, ]
dt_test <- dt_feat[dt_feat$participant > 100, ]

### Tidy Up ###
rm(ind)
```

Here we have made a simple split that all participants with id <= 100 are used for training, otherwise test.
What are benefits and drawbacks to doing so?

## Random forest training on all data

Now we build a random forest trained/tested on all our data.
```{r random-forest-all, results = 'hold'}
### Random Forest Fit ###
fit_rf <- randomForest(
  # Training Data #
  x = dt_train[, ..predvars],
  y = as.factor(dt_train[, label]),
  # Test Data #
  xtest = dt_test[, ..predvars],
  ytest = as.factor(dt_test[, label]),
  # Model Params #
  replace = T, # Should sampling of cases be done with replacement?
  strata = label, # Factor variable used for stratified sampling.
  sampsize = 1e2, # Size(s) of samples to draw.
  importance = T, # Should variable importance be assessed?
  keep.forest = T # Keep final forest for predictions.
)

### Confusion Matrix ###
fit_rf

```

Note, this model has several parameters set to values you may wish to tweak later:

* `ntree = 500` --- the number of trees to grow
* `mtry = if (!is.null(y) && !is.factor(y)) max(floor(ncol(x)/3), 1) else floor(sqrt(ncol(x)))` --- the number of variables randomly sampled at each split.

The out-of-bag (OOB) & per category misclassification rates with increasing number of trees grown:

```{r random-forest-errors, results = 'hold'}
### Plot ###
plot(
  x = fit_rf, # Model object.
  main = 'Random Forest Misclassification Rate', # Title.
  ylim = c(0, 1), # y-axis limits.
  bty = 'n' # Remove outer box.
)

### Legend ###
legend(
  x = fit_rf[['ntree']], y = 0.65, # x,y coordinates.
  xjust = 1, yjust = 1, # Legend justification.
  legend = colnames(fit_rf[['err.rate']]), # Legend labels.
  col = seq_len(ncol(fit_rf[['err.rate']])), # Legend colours.
  lty = seq_len(ncol(fit_rf[['err.rate']])), # Legend line types.
  title = 'Outcome', # Legend title.
  cex = 0.5, # Legend size scaling factor.
  bty = 'n' # Remove box around legend.
)
```

In this case, the plot shows there doesn't appear to be much benefit in growing more than about 50 trees. Overall, the model seems to do well in distinguishing between very inactive periods (`"Sedentary"` and `"Sleep"`), but there seems to be confusion between the remaining activities. Our ability to classify walking using our extracted metrics is awful, with a mean misclassification rate hovering at around `r round(fit_rf[['confusion']]['Walking', 'class.error'], 2)`.

## Plot predicted vs. true activity profiles

```{r random-forest-calibration, results = 'hold'}
### Observed & Predicted ###
obs <- dt_test[, label]
pred <- predict(fit_rf, dt_test)

### Plot ###
## Palette ##
pal <- viridis(length(unique(obs)))

### Order table ###
tbl <- table(obs, pred)
tbl <- tbl[order(factor(rownames(tbl), levels = unique(obs))),
           order(factor(colnames(tbl), levels = unique(obs)))]

## Barplot ##
barplot(
  prop.table(tbl),
  col = pal,
  xlab = 'Observed',
  ylab = 'Proportion'
)
## Legend ##
legend(
  'topright',
  legend = unique(obs),
  col = pal,
  pch = 15,
  title = 'Predicted',
  cex = 0.8,
  bty = 'n'
)

### Tidy Up ###
rm(obs, pred, pal)
```

Importance

```{r importance, results = 'hold'}
importance(fit_rf)
varImpPlot(fit_rf, main = 'Importance')
```

<!-- HERE -->

```{r detach-packages, echo = F, results = 'hide'}
detach('package:randomForest', unload = T)
```
