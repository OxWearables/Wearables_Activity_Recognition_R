---
title: 'Introduction'
output: github_document
---

<!-- Notes -->
<!-- https://github.com/OxWearables/Oxford_Wearables_Activity_Recognition/blob/master/0_Intro.ipynb -->
<!-- 'viridisLite' package can be used if ggplot2 is not required -->


```{r setup, include = F}
### Knit Options ###
knitr::opts_knit[['set']](
  base.dir = '~/Git/Wearables_Activity_Recognition/',
  root.dir = '~/Git/Wearables_Activity_Recognition/'
)

### Chunk Options ###
knitr::opts_chunk[['set']](
  echo = T,
  comment = '',
  dpi = 600,
  warning = F,
  message = F,
  fig.path = 'figures/'
)

### General Options ###
options(
  digits = 7, # Default = 7
  scipen = 0, # Default = 0
  width = 120, # Default = 80
  timeout=5000
)
```

## Setup

Install required packages:

```{r install-packages, results = 'hide'}
### Required Packages ###
pkgs <- c(
  'data.table', # Data import/management.
  'R.utils', # Uncompressing compressed CSV files.
  'viridis', # Colour blind friendly palettes.
  'here',
  'foreach',
  'doParallel',
  'progress'
)

### Packages to Install ###
pkgs <- pkgs[!{pkgs %in% rownames(installed.packages())}]

### Install (if Required) ###
install.packages(pkgs)

### Tidy Up ###
rm(pkgs)
```

Load required package namespaces:

```{r load-packages, results = 'hide'}
### Packages ###
library(viridis)
library(data.table)
library(here)
library(foreach)
library(doParallel)
library(progress)
```

Define some useful path variables:

```{r paths}
### Paths ###
## Capture-24 Directory ##
dir_cap24 <- paste(here(), 'data/capture24', sep = '/')
## Capture-24 Zip Archive Directory ##
zip_cap24 <- paste(here(), 'data/capture24.zip', sep = '/')
## Capture-24 Sample Directory ##
dir_cap24_sample <- paste(here(), 'data/capture24_sample', sep = '/')
## Capture-24 Sample Zip Archive Directory ##
zip_cap24_sample <- paste(here(), 'data/capture24_sample.zip', sep = '/')
```

## Load & inspect the dataset

To run this notebook, you'll need the [Capture-24 dataset](https://ora.ox.ac.uk/objects/uuid:99d7c092-d865-4a19-b096-cc16440cd001).

Download the data:
```{r}
url <- "https://ora.ox.ac.uk/objects/uuid:99d7c092-d865-4a19-b096-cc16440cd001/files/rpr76f381b"
download.file(url, zip_cap24, mode = "wb", timeout = 1200)
```

Extract the data:

```{r data-extract, eval = F}
## Extract Capture24 Data ##
unzip(
  zipfile = zip_cap24,
  overwrite = F,
  junkpaths = T,
  exdir = dir_cap24,
  unzip = getOption('unzip')
)
```

Let's see what's in the extracted archive:

```{r list-files}
### Print First 10 Files ###
list.files(dir_cap24)[1:10]
```

```{r copy-samples}
dir.create(dir_cap24_sample)
file.copy(file.path(dir_cap24, 'annotation-label-dictionary.csv'),
          file.path(dir_cap24_sample, 'annotation-label-dictionary.csv'))

file.copy(file.path(dir_cap24, 'metadata.csv'),
          file.path(dir_cap24_sample, 'metadata.csv'))

file.copy(file.path(dir_cap24, 'P001.csv.gz'),
          file.path(dir_cap24_sample, 'P001.csv.gz'))
```

```{r extract-features}
# Import annotations labels
dt_dic <- fread(
  file = file.path(dir_cap24, 'annotation-label-dictionary.csv'), # File path.
  sep = ',' # Delimiter.
)

# Make Life a Little Easier for Ourselves
setnames(dt_dic, sub('^label:', '', names(dt_dic))) # Swap out special characters.

# Create New Factor Representation of the Walmsley 2020 Variable
dt_dic[, label := factor(
  x = Walmsley2020,
  levels = c('sleep', 'sedentary', 'light', 'moderate-vigorous'),
  labels = c('Sleep', 'Sedentary', 'Light', 'MVPA')
)]

# Define a mode function
mode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

# List to store all feature data tables
feats_all <- list()

# Initialise parallel loop
cl <- makeCluster(detectCores())
registerDoParallel(cl)

pb <- progress_bar$new(total = 151)

# Loop through all participants and extract features
feats_all <- foreach(i = 1:151) %dopar% {
  library('data.table')
  # Load data for participant
  dt_p <- fread(
    file = file.path(dir_cap24, sprintf('P%03d.csv.gz', i)), # File path.
    sep = ',', # Delimiter.
    na.strings = '' # Missing data encoding.
  )
  
  # Omit Rows Containing NAs
  dt_p <- dt_p[complete.cases(dt_p)]
  
  # Translate Annotations Using Walmsley's 2020 Labels
  dt_p <- merge(
    x = dt_p, # Data table `x`.
    y = dt_dic[, .(annotation, label)], # Data table `y`.
    by = 'annotation', # Join column.
    all.x = T, # Left join.
    sort = F # Don't sort by join column.
  )
  
  # Drop annotation Column
  dt_p[, annotation := NULL]
  
  # Drop Unused Factor Levels
  dt_p[, label := droplevels(label)]
  
  # Group Data into 30s Windows
  breaks <- with(dt_p, seq(min(time), max(time), by = 30))
  dt_p[, tgrp := cut(
    x = time,
    breaks = breaks,
    labels = paste0('T', seq_len(length(breaks) - 1L)),
    include.lowest = T
  )]
  setcolorder(dt_p, c('tgrp', 'time', 'label')) # Reorder columns.
  
  # Classify 30s Windows Via Majority Vote
  dt_p[, label := mode(label), by = tgrp]
  
  # Tidy Up
  rm(breaks)
  
  # Euclidean Distance Function
  linalgnorm <- function(x, y, z) {
    v <- sqrt(x^2 + y^2 + z^2)
    return(v)
  }
  
  # Compute Features
  dt_feat <- dt_p[, .(
    x_mean = mean(x), # Mean of `x`.
    x_sd = sd(x), # SD of `x`.
    y_mean = mean(y), # Mean of `y`.
    y_sd  = sd(y), # SD of `y`.
    z_mean = mean(z), # Mean of `z`
    z_sd = sd(z), # SD of `z`
    v_mean = mean(linalgnorm(x, y, z)), # Mean of the Euclidean distance.
    v_sd = sd(linalgnorm(x, y, z)), # SD of the Euclidean distance.
    label = first(label), # First label value
    participant = i
  ), by = tgrp]
    # Omit NAs
  
  dt_feat <- dt_feat[complete.cases(dt_feat)]
  
  # Drop tgrp variable
  dt_feat[, tgrp := NULL]
  
  pb.tick()
  
  # Add to list
  dt_feat
}

# Close the parallel backend
stopCluster(cl)

# Combine all participant features into one data table
dt_feats_all <- rbindlist(feats_all)

# Save dt_feats_all as a CSV file
fwrite(dt_feats_all, file = file.path(dir_cap24_sample, "feats_all.csv"), sep = ",")
```

```{r zip-samples}
zip(zip_cap24_sample, 
    list.files(dir_cap24_sample, recursive = TRUE, full.names = TRUE))
```



